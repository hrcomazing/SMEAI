"""Utility functions for text chunking and cleaning."""

# Pseudocode only - implement tokenization and cleaning logic here.

def split_into_chunks(text: str, max_tokens: int = 500):
    """Return a list of text chunks each under ``max_tokens``."""
    # TODO: add real tokenization logic
    return [text]
